{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from Utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('pairRDDExamples').setMaster(\"local[*]\")   # It will run all the available cores on local cpu\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfTuples = [('Lily', 23),('Jack', 29),('Mary', 29),('James', 8)] \n",
    "pairRDD = sc.parallelize(listOfTuples) \n",
    "outFolder = \"pairRDDfromTupleList\"\n",
    "pairRDD.coalesce(1).saveAsTextFile(outFolder)    # Coalesce returns a new RDD reduced into numPartitions (passed as argument) partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a regular RDD to pair-RDD\n",
    "inputStrings =  ['Lily 23', 'Jack 29', 'Mary 29', 'James 8']\n",
    "regularRDD = sc.parallelize(inputStrings)\n",
    "pairRDDs = regularRDD.map(lambda s: (s.split(\" \")[0], s.split(\" \")[1]))  #splits and converts the elements of string-list into tuples\n",
    "outFolder = \"pairRDDfromRDD\"\n",
    "pairRDDs.coalesce(1).saveAsTextFile(outFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranformations on Pair RDDs\n",
    "\n",
    "# Filter transformation\n",
    "filePath = 'e:\\\\Eskills-Academy-projects\\\\python-spark-tutorial-master\\\\in\\\\'\n",
    "fileName = \"airports.text\"\n",
    "textFile = filePath + fileName\n",
    "airportsRDD = sc.textFile(textFile)\n",
    "airportsPairRDD = airportsRDD.map(lambda line: (Utils.COMMA_DELIMITER.split(line)[1],\n",
    "                                                Utils.COMMA_DELIMITER.split(line)[3]))\n",
    "\n",
    "airportsNotInUSA = airportsPairRDD.filter(lambda keyValue: keyValue[1] != \"\\\"United States\\\"\")\n",
    "outFolder = \"airportsNotInUSA\"\n",
    "\n",
    "airportsNotInUSA.saveAsTextFile(outFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "upperCase = airportsPairRDD.mapValues(lambda countryName: countryName.upper())\n",
    "upperCase.saveAsTextFile(\"airportsUpperCase.text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation: reduceByKey()\n",
    "filePath = 'file:///E:/Eskills-Academy-projects/python-spark-tutorial-master/in/'\n",
    "fileName = \"word_count.text\"\n",
    "textFile = filePath + fileName\n",
    "lines = sc.textFile(textFile)\n",
    "\n",
    "wordRDD = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordPairRDD = wordRDD.map(lambda word: (word, 1))\n",
    "wordCounts = wordPairRDD.reduceByKey(lambda x, y: x + y)\n",
    "sortedWordCount =wordCounts.sortBy(lambda wordCounts: wordCounts[1], ascending=False)\n",
    "\n",
    "for word, count in wordCounts.collect():\n",
    "    print(\"{}:{}\".format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'file:///E:/Eskills-Academy-projects/python-spark-tutorial-master/in/'\n",
    "fileName = \"RealEstate.csv\"\n",
    "textFile = filePath + fileName\n",
    "lines = sc.textFile(textFile)\n",
    "linesWithoutHeader = lines.filter(lambda line: \"Bedrooms\" not in line)\n",
    "BedRoomCountIx, PriceIx = 3, 2   # Given in the csv file\n",
    "\n",
    "class AvgCount():\n",
    "    def __init__(self, count: int, total: float) -> None:\n",
    "        self.count = count\n",
    "        self.total = total\n",
    "\n",
    "housePricePairRdd = linesWithoutHeader.map(lambda line: ((int(float(line.split(\",\")[BedRoomCountIx]))),\n",
    "                                                                  AvgCount(1, float(line.split(\",\")[PriceIx]))))\n",
    "\n",
    "housePriceTotal = housePricePairRdd.reduceByKey(lambda x, y: AvgCount(x.count+y.count, x.total+y.total))\n",
    "housePriceAvg = housePriceTotal.mapValues(lambda avgCount: avgCount.total / avgCount.count)\n",
    "sortedHousingPriceAvg = housePriceAvg.sortByKey(ascending=False)\n",
    "sortedHousingPriceAvg.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by key\n",
    "filePath = 'file:///E:/Eskills-Academy-projects/python-spark-tutorial-master/in/'\n",
    "fileName = \"airports.text\"\n",
    "textFile = filePath + fileName\n",
    "lines = sc.textFile(textFile)\n",
    "\n",
    "countryAndAirportNamePair = lines.map(lambda airport:\n",
    "                                     (Utils.COMMA_DELIMITER.split(airport)[3],\n",
    "                                      Utils.COMMA_DELIMITER.split(airport)[1]))\n",
    "\n",
    "airportsByCountry = countryAndAirportNamePair.groupByKey()\n",
    "# airportsByCountry.take(6)\n",
    "for country, airportName in airportsByCountry.collectAsMap().items():\n",
    "    print(\"{}:{}\".format(country, list(airportName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join operations\n",
    "ages = sc.parallelize([(\"Tom\", 29), (\"John\", 22)])\n",
    "addresses = sc.parallelize([(\"James\", \"USA\"),(\"John\", \"UK\")])\n",
    "join = ages.join(addresses)\n",
    "leftOuterJoin = ages.leftOuterJoin(addresses)\n",
    "rightOuterJoin = ages.rightOuterJoin(addresses)\n",
    "fullOuterJoin = ages.fullOuterJoin(addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', (22, 'UK'))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join.collect()[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', (22, 'UK')), ('Tom', (29, None))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftOuterJoin.collect()[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', (22, 'UK')), ('James', (None, 'USA'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rightOuterJoin.collect()[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', (22, 'UK')), ('James', (None, 'USA')), ('Tom', (29, None))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullOuterJoin.collect()[:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
