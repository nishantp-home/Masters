# Classification of fashion MNIST data
# An MNIST-like dataset of 70,000 28x28 labeled fashion images
# Labels: 0 T-shirt/top, 1 Trouser, 2 Pullover, 3 Dress, 4 Coat, 5 Sandal, 6 Shirt, 7 Sneaker, 8 Bag, 9 Ankle boot
# Each row is a separate image. Column 1 is the class label. Remaining columns are pixel numbers (784 total).
# Each value is the darkness of the pixel (1 to 255)

# Specify global file path, where data set is located
```{r}
file_path <- "E:\\Eskills-Academy-projects\\fashion-mnist\\"
train_file_name <- "fashion-mnist_train.csv"
train_file <- paste(file_path, train_file_name, sep = "")
test_file_name <- "fashion-mnist_test.csv"
test_file <- paste(file_path, test_file_name, sep = "")
```
# Load data
```{r}
f_data <- read.csv(train_file)
```
```{r}
head(f_data)
```
```{r}
str(f_data)
ncol(f_data)
nrow(f_data)
```
# Convert labels from int to factor for Classification
```{r}
f_data$label <- as.factor(f_data$label)
str(f_data)
levels(f_data$label)
```

# Load H2O library and initiate an h2o object with 2 threads 
```{r}
library(h2o)
h2o.init(
  max_mem_size = "2G",
  nthreads = 2,
  ip = "localhost",
  port = 54321
)
```

# Convert data into h2o compatible hex file
```{r}
d_hex <- as.h2o(f_data, destination_frame = "d_hex")
```

```{r}
head(d_hex)
```

# Split hex data into train and test datasets
```{r}
g_split <- h2o.splitFrame(data = d_hex, ratios = 0.75)
train <- g_split[[1]]
test <- g_split[[2]]
```
```{r}
head(train)
dim(train)
```
```{r}
head(test)
dim(test)
```

# Implement a DNN with 3 hidden layers (1000, 1000 and 2000 neurons)
# Activation function : Rectifier with dropout (improves generalization)
# epochs: The number of iterations to be carried out.
## Adaptive learning rates self adjust to avoid local minima or slower convergence
# Momentum modifies back-propagation by allowing prior iterations to influence the current update.
```{r}
fmnist_nn <- h2o.deeplearning(
  x = 2:785, # Column indices for predictors
  y = "label", # Name of column containing response variable
  training_frame = train,
  distribution = "multinomial",
  model_id = "fmnist_nn",
  activation = "RectifierWithDropout",
  hidden = c(1000, 1000, 2000),
  epochs = 1, # training with 180 epochs yields good accuracy, but takes hours
  adaptive_rate = FALSE,
  rate = 0.01,
  rate_annealing = 1.0e-6,
  rate_decay = 1.0,
  momentum_start = 0.4,
  momentum_ramp = 384000,
  momentum_stable = 0.98,
  input_dropout_ratio = 0.22,
  l1 = 1.0e-5,
  max_w2 = 15.0,
  initial_weight_distribution = "Normal",
  initial_weight_scale = 0.01,
  nesterov_accelerated_gradient = TRUE,
  loss = "CrossEntropy",
  fast_mode = TRUE,
  diagnostics = TRUE,
  ignore_const_cols = TRUE,
  force_load_balance = TRUE,
  seed = 3.656455e+18
)
```

```{r}
h2o.confusionMatrix(fmnist_nn, test)
# This gives an accuracy ~ 83%, (1 epoch)
```


# For more efficiency, train with lesser hidden layers with lesser neuron count 
# Default: 2 hidden layers with 200 neurons each. 
```{r}
fmnist_nn2 <- h2o.deeplearning(
  x = 2:785, # Column indices for predictors
  y = "label", # Name of column containing response variable
  training_frame = train,
  distribution = "multinomial",
  model_id = "fmnist_nn2",
  activation = "Tanh",
  l2 = 0.00001,
  loss = "CrossEntropy",
  epochs = 5
)
```

```{r}
h2o.confusionMatrix(fmnist_nn2, test)
# Accuracy of 85% (1 epoch), 86% (2 epochs), 87% (5 epochs)
```